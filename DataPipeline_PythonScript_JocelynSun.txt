####################
# 1- import packages
####################
import sys
import os
#from pathlib import Path
import json
import glob
import pandas as pd
import pyodbc


########################################
# 2- main entry function                   
########################################
#This snippet defines a main entry function for a script that can be called from the command line or CI/CD pipeline.
def main(env: str | None = None) -> int:
    """
    main entry function is the safe entrypoint for testing/CICD purposes. When called with no `env`, the function
    will read the value from sys.argv[1] (to preserve original CLI behavior).

    Returns an integer exit code (0 success, non-zero error).
    """
    source = "Demo_Funnel"

    ####################################################
    # 3- Environment selection 
    ####################################################
    # If env wasn't passed programmatically, take from CLI args (preserve original behavior)
    if env is None:
        # Check if the user provided the environment argument
        # len(sys.argv) < 2 checks whether the user provided at least one argument after the script name.
        if len(sys.argv) < 2:
            print("âŒ ERROR: Invalid prompt. Prompt must look like the following: python <script.py> <environment> --EXAMPLE: python DemoData.py dev")
            return 1
        env = sys.argv[1]  # "dev", "qa", or "prod"

    ########################################################################
    # 4- path to config.json file in config folder (cross-platform compatible)
    #########################################################################
    # This moves up two levels from the fileâ€™s location. So base_dir = the root folder of your project.
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    # This joins paths in a cross-platform way. It creates the full path to config.json.(e.g., /path/to/your/project/config/config.json)
    config_path = os.path.join(base_dir, "config", "config.json")
    """
    # Alternatively, you can use pathlib for path manipulations:
    from pathlib import Path
    repo_root = Path(__file__).resolve().parents[1] # Move up two levels to get the repo root
    config_path = repo_root / "config" / "config.json"
    """

    # This opens the config.json file and loads its contents into the config variable as a Python dictionary.
    # "with" is a context manager: automatically closes the file after reading, even if an error occurs.
    with open(config_path) as f:
        config = json.load(f)
        print("â„¹ï¸  Loading SQL connection values from project config.json file. Look at config.json file to see current destination location. Edit config.json file if necessary.")

    # This block verifies if the environment (env) exists in the configuration. If not, prints a clear error and stops execution with an error code.
    if env not in config:
        print(f"âŒ ERROR: Invalid environment '{env}'. Environment must be one of the following: {list(config.keys())}")
        return 1

    # Extracts the server, database, and table information for the specified environment from the config dictionary.
    server = config[env]["server"]
    database = config[env]["database"]
    table = config[env]["tables"][source]


    ######################################################################
    # 5 - path to CSV file in the import folder (must be exactly 1 file)
    ######################################################################

    # This uses glob to find all CSV files in the import directory.
    import_dir = os.path.join(base_dir, "import")
    csv_files = glob.glob(os.path.join(import_dir, "*.csv"))

    # This checks if there are zero or multiple CSV files and handles those cases with error messages and exit codes.
    if len(csv_files) == 0:
        print("âŒ ERROR: No CSV file found in the import folder.")
        return 1
    elif len(csv_files) > 1:
        print("âŒ ERROR: More than one CSV file found in the import folder. Please import only one at a time.")
        return 1

    # If exactly one CSV file is found, it selects that file and prints its name.
    csv_file = csv_files[0]
    print(f"ðŸ“‚ Using import file: {os.path.basename(csv_file)}")

    # load the CSV file into a pandas DataFrame for processing.
    df = pd.read_csv(csv_file)


    ###################################################
    # 6 - rename csv columns to match SQL table (in df)
    ###################################################
    df.rename(columns={
       "TransID": "TransactionID",
       "Customer Short name": "CustomerShortname",
        "Products": "Product", 
        "Amount": "Amount",
        "Time": "Time",
        "YYMMDD": "Date",         
    }, inplace=True)


    ##################################################
    # 7 - check for extra column(s) or missing column(s)
    ##################################################
    expected_columns = list(rename_map.values())

    actual_columns = df.columns.tolist()

    missing = [col for col in expected_columns if col not in actual_columns]
    extra   = [col for col in actual_columns if col not in expected_columns]

    if missing:
        print()
        print("ERROR: The CSV file is missing the following required column(s):")
        for col in missing:
            print(f"   - {col}")
        print()
        sys.exit(1)

    if extra:
        print()
        print("ERROR: The CSV file has unexpected extra column(s):")
        for col in extra:
            print(f"   - {col}")
        print()
        sys.exit(1)
    
    ##########################################################
    # 8 - connect to SQL Server using Windows Authentication
    ##########################################################
    conn = pyodbc.connect(
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"Trusted_Connection=yes;"
    )
    # create cursor
    cursor = conn.cursor()


    ##################################################################
    # 9 - fetch existing unique keys of column combination from DB
    ##################################################################
    cursor.execute(f"""
        SELECT TransactionID, CustomerShortname, Product
        FROM {table}
    """)
    existing_keys = set(
        (
            str(row[0]) if row[0] is not None else None,
            str(row[1]) if row[1] is not None else None,
            str(row[2]) if row[2] is not None else None
        )
        for row in cursor.fetchall()
    )


    ##################################################################
    # 10 - Rows to insert (filter out duplicates from DataFrame)
    ##################################################################
    rows_to_insert = df[
        df.apply(
            lambda x: (
                str(x['TransactionID']),
                str(x['CustomerShortname']) if pd.notna(x['CustomerShortname']) else None,
                str(x['Product'])
            ) not in existing_keys,
            axis=1
        )
    ]

    print("============================================")
    print(f"Total rows in CSV: {len(df)}")
    print(f"CSV rows already exist in the destination table: {len(df) - len(rows_to_insert)}")
    print(f"CSV rows need to be inserted: {len(rows_to_insert)}")
    print("============================================")


    ################################################################################
    # 11 - prepare data that will be inserted, ensure correct data types for each column
    ################################################################################ 
    data_to_insert = [
        (
            str(row['TransactionID']),
            str(row['CustomerShortname']) if pd.notna(row['User_ID']) else None,
            str(row['Product']),
            str(row['Amount']) if pd.notna(row['Amount']) else None,
            pd.to_datetime(row['Time']) if pd.notna(row['Time']) else None,
            pd.to_datetime(row['Date'])
        )
        for _, row in rows_to_insert.iterrows()
    ]        
        
    ##############################
    # 12 - perform bulk insert
    ##############################
    # executemany() allows you to insert/update multiple rows in a single operation instead of each row individually
    if data_to_insert:
        cursor.fast_executemany = True
        cursor.executemany(
            f"""
            INSERT INTO {table} (
                TransactionID, CustomerShortname, Product, Amount, Time,Date
            ) VALUES (?, ?, ?, ?, ?)
            """,
            data_to_insert
        )
        print(f"âœ… Inserted {len(data_to_insert)} new rows.")
    else:
        print("â„¹ï¸  No new rows to insert.")


    ##################################################
    # 13 - commit transactions and close connection
    ##################################################
    conn.commit() # --> this ensures transactions are permanent (possibility of transactions getting rolled back without this)
    cursor.close()
    conn.close()


    ####################################
    # 14 - elete CSV after processing
    ####################################
    try:
        os.remove(csv_file)
        print(f"Deleted processed file: {os.path.basename(csv_file)}")
    except Exception as e:
        print(f"âš ï¸ Warning: Could not delete {os.path.basename(csv_file)} -> {e}")

    return 0


if __name__ == "__main__":
    # Exit with the code returned by main() so the script behaves the same in CLI/CI.
    raise SystemExit(main())